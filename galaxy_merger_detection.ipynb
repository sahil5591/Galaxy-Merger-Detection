{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kernel.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "_uuid": "4a0fcc96e49162ba6931c97202af93010e099718",
        "id": "HfPFYxvW8u5M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create validation data"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4cbc4897e79f9a83bd13f10d8936383903d7cde9",
        "id": "bArIAudZ8u5P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import glob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b2eda6ef7bfb0db9993c586f02be9de288d827f4",
        "id": "kvtSnblU8u5Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_path = '../input/'\n",
        "TRAIN = '../input/images_training_rev1/images_training_rev1'\n",
        "TEST = '../input/images_test_rev1/images_test_rev1'\n",
        "LABELS = '../input/training_solutions_rev1/training_solutions_rev1.csv'\n",
        "# g = glob.glob(data_path+'train/*.jpg')\n",
        "# shuf = np.random.permutation(g)\n",
        "# for i in range(2000):\n",
        "#     os.rename(shuf[i], data_path+ 'valid/' + shuf[i].split(\"/\")[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "949cea9ebbedc0a8d170ba2c13e85f7e82175286",
        "id": "67DJIMAo8u5c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building VGG model in Keras"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90631c5686d8fc757da85495130396dc5861ad84",
        "id": "Wjumi_3o8u5e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers.core import Flatten, Dense, Dropout, Lambda, Reshape\n",
        "from keras.layers import Input, MaxoutDense\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation\n",
        "from keras.optimizers import SGD, RMSprop, Adam\n",
        "from keras import applications\n",
        "import matplotlib.pylab as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0ffca860f2c5741c89683fc05eb5891c0445a5ec",
        "id": "f6dYE29F8u5j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Compile \n",
        "optimizer = Adam(lr=1e-5)\n",
        "model = applications.VGG16(weights='imagenet', include_top= False, input_shape=(212,212, 3))\n",
        "#Adding custom Layers \n",
        "\n",
        "x = model.output\n",
        "x = Flatten()(x)\n",
        "# x = Dense(100, activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(37, activation='sigmoid')(x)\n",
        "\n",
        "# creating the final model \n",
        "model = Model(input = model.input, output = predictions)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy', 'mse'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7e3acc7eb143c505bae188d96201c537725d268a",
        "id": "-eC_ej7K8u5o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from random import shuffle\n",
        "from scipy.misc import imresize  \n",
        "\n",
        "class data_getter:    \n",
        "    \"\"\"\n",
        "    Creates a class for handling train/valid/test data paths,\n",
        "    training labels and image IDs.\n",
        "    Useful for switching between sample and full datasets.\n",
        "    \"\"\"\n",
        "    def __init__(self, path):    \n",
        "        self.path = path \n",
        "        self.train_path = TRAIN\n",
        "        #self.val_path = path + \"valid\"\n",
        "        self.test_path = TEST\n",
        "        \n",
        "        def get_paths(directory):\n",
        "            return [f for f in os.listdir(directory)]\n",
        "        \n",
        "        self.training_images_paths = get_paths(self.train_path)\n",
        "        #self.validation_images_paths = get_paths(self.val_path)\n",
        "        self.test_images_paths = get_paths(self.test_path)    \n",
        "        \n",
        "        def get_all_solutions():\n",
        "        ### Import solutions file and load into self.solutions\n",
        "            import csv\n",
        "            all_solutions = {}\n",
        "            #/'training_solutions_rev1.csv'\n",
        "            with open(LABELS, 'r') as f:\n",
        "                reader = csv.reader(f, delimiter=\",\")\n",
        "                next(reader)\n",
        "                for i, line in enumerate(reader):\n",
        "                    all_solutions[line[0]] = [float(x) for x in line[1:]]\n",
        "            return all_solutions\n",
        "        \n",
        "        self.all_solutions = get_all_solutions()\n",
        "\n",
        "    def get_id(self,fname):\n",
        "        return fname.replace(\".jpg\",\"\").replace(\"data\",\"\")\n",
        "        \n",
        "    def find_label(self,val):\n",
        "        return self.all_solutions[val]\n",
        "        \n",
        "# fetcher = data_getter('data/sample/')\n",
        "fetcher = data_getter(data_path)\n",
        "print(fetcher.train_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3cf2d5280454eeb454460f4573002c6c41c8a8ca",
        "id": "VOIU8ybZ8u5s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## from keras.preprocessing.image import ImageDataGenerator\n",
        "from skimage.filters import gaussian \n",
        "import random \n",
        "from skimage.transform import resize\n",
        "\n",
        "IDG = ImageDataGenerator()\n",
        "def process_images(paths):\n",
        "    \"\"\"\n",
        "    Import image at 'paths', decode, centre crop and prepare for batching. \n",
        "    \"\"\"\n",
        "    count = 2*len(paths)\n",
        "    length_paths = len(paths)\n",
        "    arr = np.zeros(shape=(count,3,212,212))\n",
        "    \n",
        "    theta = int(np.pi / 180 * np.random.uniform(-90.0, 90))\n",
        "    \n",
        "    fh = bool(random.getrandbits(1))\n",
        "    fv = bool(random.getrandbits(1))\n",
        "    \n",
        "    transform_parameters ={ 'theta': theta, 'flip_horizontal': fh,  'flip_vertical': fv }\n",
        "    \n",
        "    for c, path in enumerate(paths):\n",
        "        img = plt.imread(path).T\n",
        "        img = img[:,106:106*3,106:106*3] #crop 424x424 -> 212x212\n",
        "        # img = imresize(img,size=(106,106,3),interp=\"cubic\").T # downsample to half res\n",
        "        arr[c] = img\n",
        "        img = IDG.apply_transform(img, transform_parameters)\n",
        "        img = gaussian(img, sigma=0.5)\n",
        "        arr[c + length_paths] = img\n",
        "    return arr\n",
        "\n",
        "def no_process_images(paths):\n",
        "        img = plt.imread(path).T\n",
        "        img = img[:,106:106*3,106:106*3] #crop 424x424 -> 212x212\n",
        "\n",
        "        img = gaussian(img, sigma=0.5)\n",
        "        arr[c] = img\n",
        "    return arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ceee5cf4e0446c317d31221e5d2c4f83912aca78",
        "id": "5Rptl99E8u5u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Print some before/after processing images\n",
        "\n",
        "#process_images([fetcher.train_path + '/' + fetcher.training_images_paths[100]])\n",
        "im = plt.imread(fetcher.train_path + '/' + fetcher.training_images_paths[3])\n",
        "# print(im.shape)\n",
        "theta = int(np.pi / 180 * np.random.uniform(-90.0, 90))\n",
        "fh = bool(random.getrandbits(1))\n",
        "fv = bool(random.getrandbits(1))\n",
        "transform_parameters ={'theta': theta, 'flip_horizontal': fh,  'flip_vertical': fv }\n",
        "\n",
        "plt.imshow(im)\n",
        "plt.show()\n",
        "# im = im.T[:,106:106*3,106:106*3] #crop 424x424 -> 212x212\n",
        "im = im.T\n",
        "im = IDG.apply_transform(im, transform_parameters)\n",
        "im = gaussian(im, sigma=0.5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9e3d7d9ccf7eff35ae834ffbefee6dffbb5895b9",
        "id": "PyMlfZt48u5y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create generator that yields (current features X, current labels y)\n",
        "def BatchGenerator(getter):\n",
        "    while 1:\n",
        "        for f in getter.training_images_paths:\n",
        "            X_train = process_images([getter.train_path + '/' + fname for fname in [f]])\n",
        "            X_train = np.reshape(X_train, (212, 212, 3, 2))\n",
        "            id_ = getter.get_id(f)\n",
        "            y_train = np.array(getter.find_label(id_))\n",
        "            y_train = np.reshape(y_train,(1,37))\n",
        "            y_train = np.hstack((y_train, y_train))\n",
        "            yield (X_train, y_train)\n",
        "            \n",
        "def NoBatchGenerator(getter):\n",
        "    while 1:\n",
        "        for f in getter.training_images_paths:\n",
        "            X_train = no_process_images([getter.train_path + '/' + fname for fname in [f]])\n",
        "            X_train = np.reshape(X_train, (1, 212, 212, 3))\n",
        "            id_ = getter.get_id(f)\n",
        "            y_train = np.array(getter.find_label(id_))\n",
        "            y_train = np.reshape(y_train,(1,37))\n",
        "            # y_train = np.hstack((y_train, y_train, y_train, y_train))\n",
        "            yield (X_train, y_train)\n",
        "            \n",
        "def ValBatchGenerator(getter):\n",
        "    while 1:\n",
        "        for f in getter.training_images_paths:\n",
        "            X_train = process_images([getter.train_path + '/' + fname for fname in [f]])\n",
        "            id_ = getter.get_id(f)\n",
        "            y_train = np.array(getter.find_label(id_))\n",
        "            y_train = np.reshape(y_train,(1,37))\n",
        "            yield (X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ffc24d1a4d2e7f13ae14050ec84d67627c3f98e5",
        "id": "CQul2BFT8u53",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "85cefa050c9c737f0ac1de1ae47cc86f66d048e9",
        "id": "bCyowM1j8u55",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import Callback\n",
        "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
        "\n",
        "class LossHistory(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.acc = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.acc.append(logs.get('acc'))\n",
        "    \n",
        "early_stopping = EarlyStopping(monitor='loss', patience=7, verbose=1, mode='auto')\n",
        "history = LossHistory()\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "checkpointer = ModelCheckpoint(filepath='tmp/weights.hdf5', verbose=1, save_best_only=True)\n",
        "\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "reduceLR = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=2, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
        "\n",
        "batch_size = 32\n",
        "steps_to_take = int(len(fetcher.training_images_paths)/batch_size)\n",
        "#val_steps_to_take = int(len(fetcher.validation_images_paths)/batch_size)\n",
        "                #typically be equal to the number of unique samples if your dataset\n",
        "                #divided by the batch size.\n",
        "        \n",
        "        \n",
        "#model = load_model('tmp/weights.hdf5')\n",
        "\n",
        "hist = model.fit_generator(BatchGenerator(fetcher),\n",
        "                    samples_per_epoch=steps_to_take, \n",
        "                    nb_epoch=12,\n",
        "                    verbose=1,\n",
        "                    callbacks=[history,checkpointer, reduceLR],\n",
        "                   )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3d18739a6a8e2a6fd9b13a05b39645c414dd3531",
        "id": "UGO5DTtT8u59",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Plot training/validation loss"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed32d758e0421584e2e0348c6001763a367b8d5c",
        "id": "wSzoUXn2KCG4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(hist.epoch,hist.history['acc'],label='Test')\n",
        "# plt.plot(hist.epoch,hist.history['val_loss'],label='Validation',linestyle='--')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "73cfe4a4663e0137e441448ea5538cfae230419b",
        "id": "Em9velJ98u5-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(hist.epoch,hist.history['loss'],label='Test')\n",
        "# plt.plot(hist.epoch,hist.history['val_loss'],label='Validation',linestyle='--')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "59de062204e72f227d2f6dc7296a0d401bdc8a68",
        "id": "M3T9u41H8u6A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model Predict"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "de0bce34dbb1ed8f6214ba0743a008e09bac9f4e",
        "id": "PNbAnz158u6B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Load best model weights\n",
        "# from keras.models import load_model\n",
        "# model = load_model('tmp/weights.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bac26fa2d50c11ba4f16c156d3a0a7db9dc679e3",
        "id": "o0UV6sRZ8u6D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def TestBatchGenerator(getter):\n",
        "    while 1:\n",
        "        for f in getter.test_images_paths:\n",
        "            X_train = process_images([getter.test_path + '/' + fname for fname in [f]])\n",
        "            X_train = np.reshape(X_train, (1, 212, 212, 3))\n",
        "            yield (X_train)\n",
        "\n",
        "predictions = model.predict_generator(TestBatchGenerator(fetcher),\n",
        "                       val_samples = len(fetcher.test_images_paths),\n",
        "                        max_q_size = 32, use_multiprocessing=True, verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b79ca063d4a4590bcd4ef2b37ae722f6577110a5",
        "id": "Ja4-oQCG8u6F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "34b113256066527c962327105e2cf02055891990",
        "id": "W6a-IuvA8u6G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "header = open('../input/all_zeros_benchmark/all_zeros_benchmark.csv','r').readlines()[0]\n",
        "\n",
        "with open('submission_1.csv','w') as outfile:\n",
        "    outfile.write(header)\n",
        "    for i in range(len(fetcher.test_images_paths)):\n",
        "        id_ = (fetcher.get_id(fetcher.test_images_paths[i]))\n",
        "        pred = predictions[i]\n",
        "        outline = id_ + \",\" + \",\".join([str(x) for x in pred])\n",
        "        outfile.write(outline + \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}